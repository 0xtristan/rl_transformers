{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "env  =  gym.make('CartPole-v0')\n",
    "\n",
    "games = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    observations = []\n",
    "    obs = env.reset()\n",
    "    observations.append(obs)\n",
    "    for t in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        \n",
    "        if done:\n",
    "#             print(f'Episode {i} finished at time {t}')\n",
    "            break\n",
    "    games.append(np.stack(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "WINDOW_SIZE = BATCH_SIZE\n",
    "\n",
    "train_examples = tf.data.Dataset.from_generator(lambda: games, output_types=tf.float32)\n",
    "train_dataset = train_examples.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=(None,None))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 47, 4), dtype=float32, numpy=\n",
       "array([[[ 0.0367892 ,  0.04731073,  0.0035294 ,  0.0199406 ],\n",
       "        [ 0.03773542, -0.14786166,  0.00392821,  0.313735  ],\n",
       "        [ 0.03477819,  0.04720411,  0.01020291,  0.0222935 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.01173664, -0.02157126, -0.01463053, -0.01633503],\n",
       "        [-0.01216807, -0.21648037, -0.01495723,  0.27169612],\n",
       "        [-0.01649768, -0.02114822, -0.00952331, -0.02566659],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.00840801, -0.04120937, -0.01289579,  0.03229893],\n",
       "        [-0.00923219, -0.23614404, -0.01224981,  0.32088536],\n",
       "        [-0.01395507, -0.43108943, -0.0058321 ,  0.6096801 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00711163, -0.01575334,  0.03641624, -0.00852416],\n",
       "        [-0.0074267 , -0.21137811,  0.03624575,  0.29542246],\n",
       "        [-0.01165426, -0.40699753,  0.0421542 ,  0.5993128 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.04163404,  0.00737276,  0.03947278, -0.00348065],\n",
       "        [-0.04148659,  0.20190704,  0.03940316, -0.28345278],\n",
       "        [-0.03744845,  0.39644548,  0.03373411, -0.5634524 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.00796488, -0.03563007,  0.04545318, -0.01221677],\n",
       "        [-0.00867748, -0.23137341,  0.04520885,  0.29445362],\n",
       "        [-0.01330495, -0.03692417,  0.05109792,  0.01636467],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query*key gives the relevance measure - the better matched a key is to the query the higher it will activate and thus it will feature more in the v output\n",
    "# V[Q*K]\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "                    to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)    # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(k.shape[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)    \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)    # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "                \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)    # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)    # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)    # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)    # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)    # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)    # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])    # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)    # (batch_size, seq_len_q, d_model)\n",
    "                \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 10, 32]), TensorShape([1, 1, 10, 10]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPR_DIM = 32\n",
    "SEQ_LEN = 10\n",
    "\n",
    "temp_mha = MultiHeadAttention(d_model=REPR_DIM, num_heads=1)\n",
    "y = tf.random.uniform((1, SEQ_LEN, REPR_DIM))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "\n",
    "#         self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "#         # Decoder self-attention\n",
    "#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn1 = self.dropout1(attn1, training=training)\n",
    "#         out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "#         # Encoder-Decoder attention (v,k,q)\n",
    "#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn2 = self.dropout2(attn2, training=training)\n",
    "#         out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "#         ffn_output = self.dropout3(ffn_output, training=training)\n",
    "#         out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant of the decoder for decoder-only architectures like GPT-2 where we forgo an encoder\n",
    "class DecoderOnlyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderOnlyLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # Decoder masked self-attention\n",
    "        attn, attn_weights_block = self.mha(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        out = self.layernorm1(attn + x)\n",
    "\n",
    "        ffn_output = self.ffn(out)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out_final = self.layernorm2(ffn_output + out)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out_final, attn_weights_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderOnlyLayer(REPR_DIM, 1, REPR_DIM*4)\n",
    "\n",
    "sample_decoder_layer_output, _ = sample_decoder_layer(tf.random.uniform((64, SEQ_LEN, REPR_DIM)), False, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "$P E_{(p o s, 2 i)}=\\sin \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$\n",
    "\n",
    "$P E_{(p o s, 2 i+1)}=\\cos \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderOnlyLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "#         x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :] # would concat be better?\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, training, look_ahead_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10, 32]), TensorShape([64, 1, 10, 10]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=1, d_model=REPR_DIM, num_heads=1, \n",
    "                         dff=REPR_DIM*4, maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer1_block1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, pe_target, rate)\n",
    "\n",
    "    def call(self, tar, training, look_ahead_mask):\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, training, look_ahead_mask)\n",
    "\n",
    "        return dec_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=1, d_model=REPR_DIM, num_heads=1, dff=REPR_DIM*4, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, training=False, look_ahead_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask[:,:,0]\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "\n",
    "    return look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "#                            optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(tar):\n",
    "    tar_inp = tar[:, :-1] # batch, time, dim\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    look_ahead_mask = create_masks(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(tar_inp, \n",
    "                                     True, \n",
    "                                     look_ahead_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "#     train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = 4\n",
    "transformer = Transformer(num_layers=1, d_model=OBS_DIM, num_heads=1, dff=OBS_DIM*4, pe_target=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.1889\n",
      "Time taken for 1 epoch: 1.2491 secs\n",
      "\n",
      "Epoch 2 Loss 0.1818\n",
      "Time taken for 1 epoch: 0.0246 secs\n",
      "\n",
      "Epoch 3 Loss 0.1814\n",
      "Time taken for 1 epoch: 0.0240 secs\n",
      "\n",
      "Epoch 4 Loss 0.1855\n",
      "Time taken for 1 epoch: 0.0246 secs\n",
      "\n",
      "Epoch 5 Loss 0.1824\n",
      "Time taken for 1 epoch: 0.0251 secs\n",
      "\n",
      "Epoch 6 Loss 0.1827\n",
      "Time taken for 1 epoch: 0.0249 secs\n",
      "\n",
      "Epoch 7 Loss 0.1804\n",
      "Time taken for 1 epoch: 0.0240 secs\n",
      "\n",
      "Epoch 8 Loss 0.1791\n",
      "Time taken for 1 epoch: 0.0272 secs\n",
      "\n",
      "Epoch 9 Loss 0.1775\n",
      "Time taken for 1 epoch: 0.0250 secs\n",
      "\n",
      "Epoch 10 Loss 0.1729\n",
      "Time taken for 1 epoch: 0.0299 secs\n",
      "\n",
      "Epoch 11 Loss 0.1753\n",
      "Time taken for 1 epoch: 0.0277 secs\n",
      "\n",
      "Epoch 12 Loss 0.1746\n",
      "Time taken for 1 epoch: 0.0252 secs\n",
      "\n",
      "Epoch 13 Loss 0.1730\n",
      "Time taken for 1 epoch: 0.0244 secs\n",
      "\n",
      "Epoch 14 Loss 0.1732\n",
      "Time taken for 1 epoch: 0.0242 secs\n",
      "\n",
      "Epoch 15 Loss 0.1707\n",
      "Time taken for 1 epoch: 0.0248 secs\n",
      "\n",
      "Epoch 16 Loss 0.1686\n",
      "Time taken for 1 epoch: 0.0261 secs\n",
      "\n",
      "Epoch 17 Loss 0.1675\n",
      "Time taken for 1 epoch: 0.0249 secs\n",
      "\n",
      "Epoch 18 Loss 0.1641\n",
      "Time taken for 1 epoch: 0.0291 secs\n",
      "\n",
      "Epoch 19 Loss 0.1663\n",
      "Time taken for 1 epoch: 0.0277 secs\n",
      "\n",
      "Epoch 20 Loss 0.1659\n",
      "Time taken for 1 epoch: 0.0266 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, tar) in enumerate(train_dataset):\n",
    "        train_step(tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                          ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "#                                                 train_accuracy.result()\n",
    "                                                        )\n",
    "          )\n",
    "\n",
    "    print ('Time taken for 1 epoch: {:.4f} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Need to incorporate gym env 'done' as end token\n",
    "\n",
    "def evaluate(inp_sequence):\n",
    "    output = tf.expand_dims(inp_sequence, 0)\n",
    "\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        look_ahead_mask = create_masks(inp_sequence)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(inp_sequence, \n",
    "                                                     False,\n",
    "                                                     look_ahead_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.reduce_sum(tf.cast(predictions, tf.float32))==0:\n",
    "            return predictions, attention_weights \n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "#         output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_state(sequence, plot=''):\n",
    "    predicted_sequence, attention_weights = evaluate(sequence)\n",
    "\n",
    "    print('Input: {}'.format(sequence))\n",
    "    print('Predicted translation: {}'.format(predicted_sequence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sequence, predicted_sequence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_trajectory = tf.expand_dims(iter(train_dataset.take(1)).next()[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[[ 6.89184666e-03 -4.26125452e-02  4.15368602e-02  2.73510022e-03]\n",
      "  [ 6.03959570e-03 -2.38304824e-01  4.15915623e-02  3.08228642e-01]\n",
      "  [ 1.27349945e-03 -4.37994152e-02  4.77561355e-02  2.89472546e-02]\n",
      "  [ 3.97511205e-04 -2.39572540e-01  4.83350791e-02  3.36307108e-01]\n",
      "  [-4.39393939e-03 -4.51705754e-02  5.50612211e-02  5.92497028e-02]\n",
      "  [-5.29735116e-03 -2.41036996e-01  5.62462136e-02  3.68783891e-01]\n",
      "  [-1.01180905e-02 -4.67574634e-02  6.36218935e-02  9.43531245e-02]\n",
      "  [-1.10532399e-02 -2.42730871e-01  6.55089542e-02  4.06410724e-01]\n",
      "  [-1.59078576e-02 -4.38717633e-01  7.36371726e-02  7.19006121e-01]\n",
      "  [-2.46822108e-02 -2.44687662e-01  8.80172923e-02  4.50379699e-01]\n",
      "  [-2.95759626e-02 -4.40937042e-01  9.70248878e-02  7.69457936e-01]\n",
      "  [-3.83947045e-02 -6.37251019e-01  1.12414047e-01  1.09102452e+00]\n",
      "  [-5.11397235e-02 -8.33660483e-01  1.34234533e-01  1.41675925e+00]\n",
      "  [-6.78129345e-02 -1.03016484e+00  1.62569717e-01  1.74821091e+00]\n",
      "  [-8.84162337e-02 -8.37220788e-01  1.97533935e-01  1.51019609e+00]\n",
      "  [-1.05160646e-01 -1.03411102e+00  2.27737859e-01  1.85748613e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Predicted translation: [[[-0.40475094 -1.3906873   1.0385926   0.75820136]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-869862d924c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'decoder_layer1_block1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-275-856c643e7834>\u001b[0m in \u001b[0;36mpredict_state\u001b[0;34m(sequence, plot)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-274-0bed888cd2ee>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(attention, sentence, result, layer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_pt' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_state(example_trajectory, plot='decoder_layer1_block1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.equal([ 0.0000000000,  0.000000000,  0.0000000000,  0.0000000000], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rl_venv': venv)",
   "language": "python",
   "name": "python37664bitrlvenvvenvd21d88e11ee043b9809cf63723909d6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
