{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "env  =  gym.make('CartPole-v0')\n",
    "\n",
    "games = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    observations = []\n",
    "    obs = env.reset()\n",
    "    observations.append(obs)\n",
    "    for t in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        \n",
    "        if done:\n",
    "#             print(f'Episode {i} finished at time {t}')\n",
    "            break\n",
    "    games.append(np.stack(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "WINDOW_SIZE = BATCH_SIZE\n",
    "\n",
    "def filter_max_length(x, max_length=WINDOW_SIZE):\n",
    "    return tf.shape(x)[0] <= max_length\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: games, output_types=tf.float32)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset = train_dataset.cache()\n",
    "# train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes=(None,None))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 4), dtype=float32, numpy=\n",
       "array([[[-4.6847731e-02,  4.8800153e-03,  4.0906481e-02, -3.2671629e-03],\n",
       "        [-4.6750132e-02, -1.9080399e-01,  4.0841140e-02,  3.0203646e-01],\n",
       "        [-5.0566211e-02, -3.8648352e-01,  4.6881869e-02,  6.0731494e-01],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-1.3707830e-02, -4.7694836e-02,  4.4427183e-03,  2.2739721e-02],\n",
       "        [-1.4661727e-02, -2.4288023e-01,  4.8975130e-03,  3.1682107e-01],\n",
       "        [-1.9519331e-02, -4.7828365e-02,  1.1233934e-02,  2.5686653e-02],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-2.5797877e-02, -1.6835535e-02,  2.5227271e-02,  3.3857394e-02],\n",
       "        [-2.6134588e-02,  1.7791574e-01,  2.5904419e-02, -2.5076053e-01],\n",
       "        [-2.2576272e-02,  3.7265837e-01,  2.0889208e-02, -5.3516144e-01],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.4724783e-03, -2.0719782e-02, -7.5948760e-03,  2.1782385e-02],\n",
       "        [ 4.0580826e-03,  1.7451026e-01, -7.1592284e-03, -2.7328709e-01],\n",
       "        [ 7.5482880e-03, -2.0508813e-02, -1.2624971e-02,  1.7129213e-02],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 9.1351774e-03, -2.3054153e-02, -1.5060700e-02, -1.2840652e-02],\n",
       "        [ 8.6740945e-03, -2.1795692e-01, -1.5317513e-02,  2.7505267e-01],\n",
       "        [ 4.3149563e-03, -2.2619803e-02, -9.8164594e-03, -2.2421809e-02],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 2.1867296e-02, -1.5474745e-02,  4.4231402e-04,  2.9775832e-02],\n",
       "        [ 2.1557800e-02,  1.7964086e-01,  1.0378307e-03, -2.6276749e-01],\n",
       "        [ 2.5150618e-02,  3.7474799e-01, -4.2175194e-03, -5.5512291e-01],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query*key gives the relevance measure - the better matched a key is to the query the higher it will activate and thus it will feature more in the v output\n",
    "# V[Q*K]\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "                    to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)    # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(k.shape[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)    \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)    # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "                \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)    # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)    # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)    # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)    # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)    # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)    # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])    # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)    # (batch_size, seq_len_q, d_model)\n",
    "                \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 10, 32]), TensorShape([1, 1, 10, 10]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPR_DIM = 32\n",
    "SEQ_LEN = 10\n",
    "\n",
    "temp_mha = MultiHeadAttention(d_model=REPR_DIM, num_heads=1)\n",
    "y = tf.random.uniform((1, SEQ_LEN, REPR_DIM))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "\n",
    "#         self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "#         # Decoder self-attention\n",
    "#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn1 = self.dropout1(attn1, training=training)\n",
    "#         out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "#         # Encoder-Decoder attention (v,k,q)\n",
    "#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn2 = self.dropout2(attn2, training=training)\n",
    "#         out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "#         ffn_output = self.dropout3(ffn_output, training=training)\n",
    "#         out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant of the decoder for decoder-only architectures like GPT-2 where we forgo an encoder\n",
    "class DecoderOnlyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderOnlyLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # Decoder masked self-attention\n",
    "        attn, attn_weights_block = self.mha(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        out = self.layernorm1(attn + x)\n",
    "\n",
    "        ffn_output = self.ffn(out)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out_final = self.layernorm2(ffn_output + out)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out_final, attn_weights_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderOnlyLayer(REPR_DIM, 1, REPR_DIM*4)\n",
    "\n",
    "sample_decoder_layer_output, _ = sample_decoder_layer(tf.random.uniform((64, SEQ_LEN, REPR_DIM)), False, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "$P E_{(p o s, 2 i)}=\\sin \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$\n",
    "\n",
    "$P E_{(p o s, 2 i+1)}=\\cos \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderOnlyLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "#         x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :] # would concat be better?\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, training, look_ahead_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10, 32]), TensorShape([64, 1, 10, 10]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=1, d_model=REPR_DIM, num_heads=1, \n",
    "                         dff=REPR_DIM*4, maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer1_block1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, pe_target, rate)\n",
    "\n",
    "    def call(self, tar, training, look_ahead_mask):\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, training, look_ahead_mask)\n",
    "\n",
    "        return dec_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=1, d_model=REPR_DIM, num_heads=1, dff=REPR_DIM*4, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, training=False, look_ahead_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask[:,:,0]\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "\n",
    "    return look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "#                            optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(tar):\n",
    "    tar_inp = tar[:, :-1] # batch, time, dim\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    look_ahead_mask = create_masks(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(tar_inp, \n",
    "                                     True, \n",
    "                                     look_ahead_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "#     train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = 4\n",
    "transformer = Transformer(num_layers=1, d_model=OBS_DIM, num_heads=1, dff=OBS_DIM*4, pe_target=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.1694\n",
      "Time taken for 1 epoch: 1.3004 secs\n",
      "\n",
      "Epoch 2 Loss 0.1662\n",
      "Time taken for 1 epoch: 0.0637 secs\n",
      "\n",
      "Epoch 3 Loss 0.1621\n",
      "Time taken for 1 epoch: 0.0620 secs\n",
      "\n",
      "Epoch 4 Loss 0.1600\n",
      "Time taken for 1 epoch: 0.0620 secs\n",
      "\n",
      "Epoch 5 Loss 0.1568\n",
      "Time taken for 1 epoch: 0.0651 secs\n",
      "\n",
      "Epoch 6 Loss 0.1538\n",
      "Time taken for 1 epoch: 0.0612 secs\n",
      "\n",
      "Epoch 7 Loss 0.1510\n",
      "Time taken for 1 epoch: 0.0630 secs\n",
      "\n",
      "Epoch 8 Loss 0.1482\n",
      "Time taken for 1 epoch: 0.0619 secs\n",
      "\n",
      "Epoch 9 Loss 0.1464\n",
      "Time taken for 1 epoch: 0.0758 secs\n",
      "\n",
      "Epoch 10 Loss 0.1441\n",
      "Time taken for 1 epoch: 0.0667 secs\n",
      "\n",
      "Epoch 11 Loss 0.1426\n",
      "Time taken for 1 epoch: 0.0718 secs\n",
      "\n",
      "Epoch 12 Loss 0.1401\n",
      "Time taken for 1 epoch: 0.0759 secs\n",
      "\n",
      "Epoch 13 Loss 0.1389\n",
      "Time taken for 1 epoch: 0.0709 secs\n",
      "\n",
      "Epoch 14 Loss 0.1374\n",
      "Time taken for 1 epoch: 0.0728 secs\n",
      "\n",
      "Epoch 15 Loss 0.1345\n",
      "Time taken for 1 epoch: 0.0680 secs\n",
      "\n",
      "Epoch 16 Loss 0.1329\n",
      "Time taken for 1 epoch: 0.0754 secs\n",
      "\n",
      "Epoch 17 Loss 0.1318\n",
      "Time taken for 1 epoch: 0.0721 secs\n",
      "\n",
      "Epoch 18 Loss 0.1297\n",
      "Time taken for 1 epoch: 0.0741 secs\n",
      "\n",
      "Epoch 19 Loss 0.1286\n",
      "Time taken for 1 epoch: 0.0742 secs\n",
      "\n",
      "Epoch 20 Loss 0.1258\n",
      "Time taken for 1 epoch: 0.0732 secs\n",
      "\n",
      "Epoch 21 Loss 0.1254\n",
      "Time taken for 1 epoch: 0.0783 secs\n",
      "\n",
      "Epoch 22 Loss 0.1228\n",
      "Time taken for 1 epoch: 0.0758 secs\n",
      "\n",
      "Epoch 23 Loss 0.1219\n",
      "Time taken for 1 epoch: 0.0692 secs\n",
      "\n",
      "Epoch 24 Loss 0.1203\n",
      "Time taken for 1 epoch: 0.0660 secs\n",
      "\n",
      "Epoch 25 Loss 0.1192\n",
      "Time taken for 1 epoch: 0.0775 secs\n",
      "\n",
      "Epoch 26 Loss 0.1176\n",
      "Time taken for 1 epoch: 0.0665 secs\n",
      "\n",
      "Epoch 27 Loss 0.1170\n",
      "Time taken for 1 epoch: 0.0650 secs\n",
      "\n",
      "Epoch 28 Loss 0.1143\n",
      "Time taken for 1 epoch: 0.0675 secs\n",
      "\n",
      "Epoch 29 Loss 0.1139\n",
      "Time taken for 1 epoch: 0.0614 secs\n",
      "\n",
      "Epoch 30 Loss 0.1127\n",
      "Time taken for 1 epoch: 0.0612 secs\n",
      "\n",
      "Epoch 31 Loss 0.1114\n",
      "Time taken for 1 epoch: 0.0650 secs\n",
      "\n",
      "Epoch 32 Loss 0.1099\n",
      "Time taken for 1 epoch: 0.0618 secs\n",
      "\n",
      "Epoch 33 Loss 0.1081\n",
      "Time taken for 1 epoch: 0.0614 secs\n",
      "\n",
      "Epoch 34 Loss 0.1074\n",
      "Time taken for 1 epoch: 0.0650 secs\n",
      "\n",
      "Epoch 35 Loss 0.1061\n",
      "Time taken for 1 epoch: 0.0695 secs\n",
      "\n",
      "Epoch 36 Loss 0.1050\n",
      "Time taken for 1 epoch: 0.0702 secs\n",
      "\n",
      "Epoch 37 Loss 0.1042\n",
      "Time taken for 1 epoch: 0.0667 secs\n",
      "\n",
      "Epoch 38 Loss 0.1026\n",
      "Time taken for 1 epoch: 0.0622 secs\n",
      "\n",
      "Epoch 39 Loss 0.1018\n",
      "Time taken for 1 epoch: 0.0642 secs\n",
      "\n",
      "Epoch 40 Loss 0.1009\n",
      "Time taken for 1 epoch: 0.0623 secs\n",
      "\n",
      "Epoch 41 Loss 0.0993\n",
      "Time taken for 1 epoch: 0.0665 secs\n",
      "\n",
      "Epoch 42 Loss 0.0982\n",
      "Time taken for 1 epoch: 0.0716 secs\n",
      "\n",
      "Epoch 43 Loss 0.0969\n",
      "Time taken for 1 epoch: 0.0721 secs\n",
      "\n",
      "Epoch 44 Loss 0.0966\n",
      "Time taken for 1 epoch: 0.0754 secs\n",
      "\n",
      "Epoch 45 Loss 0.0952\n",
      "Time taken for 1 epoch: 0.0687 secs\n",
      "\n",
      "Epoch 46 Loss 0.0945\n",
      "Time taken for 1 epoch: 0.0694 secs\n",
      "\n",
      "Epoch 47 Loss 0.0928\n",
      "Time taken for 1 epoch: 0.0737 secs\n",
      "\n",
      "Epoch 48 Loss 0.0921\n",
      "Time taken for 1 epoch: 0.0739 secs\n",
      "\n",
      "Epoch 49 Loss 0.0908\n",
      "Time taken for 1 epoch: 0.0692 secs\n",
      "\n",
      "Epoch 50 Loss 0.0899\n",
      "Time taken for 1 epoch: 0.0784 secs\n",
      "\n",
      "Epoch 51 Loss 0.0891\n",
      "Time taken for 1 epoch: 0.0780 secs\n",
      "\n",
      "Epoch 52 Loss 0.0884\n",
      "Time taken for 1 epoch: 0.0759 secs\n",
      "\n",
      "Epoch 53 Loss 0.0872\n",
      "Time taken for 1 epoch: 0.0738 secs\n",
      "\n",
      "Epoch 54 Loss 0.0865\n",
      "Time taken for 1 epoch: 0.0718 secs\n",
      "\n",
      "Epoch 55 Loss 0.0856\n",
      "Time taken for 1 epoch: 0.0726 secs\n",
      "\n",
      "Epoch 56 Loss 0.0842\n",
      "Time taken for 1 epoch: 0.0727 secs\n",
      "\n",
      "Epoch 57 Loss 0.0836\n",
      "Time taken for 1 epoch: 0.0695 secs\n",
      "\n",
      "Epoch 58 Loss 0.0824\n",
      "Time taken for 1 epoch: 0.0695 secs\n",
      "\n",
      "Epoch 59 Loss 0.0819\n",
      "Time taken for 1 epoch: 0.0651 secs\n",
      "\n",
      "Epoch 60 Loss 0.0806\n",
      "Time taken for 1 epoch: 0.0617 secs\n",
      "\n",
      "Epoch 61 Loss 0.0799\n",
      "Time taken for 1 epoch: 0.0612 secs\n",
      "\n",
      "Epoch 62 Loss 0.0789\n",
      "Time taken for 1 epoch: 0.0654 secs\n",
      "\n",
      "Epoch 63 Loss 0.0777\n",
      "Time taken for 1 epoch: 0.0619 secs\n",
      "\n",
      "Epoch 64 Loss 0.0765\n",
      "Time taken for 1 epoch: 0.0612 secs\n",
      "\n",
      "Epoch 65 Loss 0.0760\n",
      "Time taken for 1 epoch: 0.0616 secs\n",
      "\n",
      "Epoch 66 Loss 0.0751\n",
      "Time taken for 1 epoch: 0.0645 secs\n",
      "\n",
      "Epoch 67 Loss 0.0747\n",
      "Time taken for 1 epoch: 0.0648 secs\n",
      "\n",
      "Epoch 68 Loss 0.0735\n",
      "Time taken for 1 epoch: 0.0636 secs\n",
      "\n",
      "Epoch 69 Loss 0.0722\n",
      "Time taken for 1 epoch: 0.0642 secs\n",
      "\n",
      "Epoch 70 Loss 0.0718\n",
      "Time taken for 1 epoch: 0.0695 secs\n",
      "\n",
      "Epoch 71 Loss 0.0706\n",
      "Time taken for 1 epoch: 0.0627 secs\n",
      "\n",
      "Epoch 72 Loss 0.0700\n",
      "Time taken for 1 epoch: 0.0611 secs\n",
      "\n",
      "Epoch 73 Loss 0.0689\n",
      "Time taken for 1 epoch: 0.0647 secs\n",
      "\n",
      "Epoch 74 Loss 0.0680\n",
      "Time taken for 1 epoch: 0.0656 secs\n",
      "\n",
      "Epoch 75 Loss 0.0672\n",
      "Time taken for 1 epoch: 0.0641 secs\n",
      "\n",
      "Epoch 76 Loss 0.0665\n",
      "Time taken for 1 epoch: 0.0719 secs\n",
      "\n",
      "Epoch 77 Loss 0.0661\n",
      "Time taken for 1 epoch: 0.0705 secs\n",
      "\n",
      "Epoch 78 Loss 0.0652\n",
      "Time taken for 1 epoch: 0.0765 secs\n",
      "\n",
      "Epoch 79 Loss 0.0644\n",
      "Time taken for 1 epoch: 0.0688 secs\n",
      "\n",
      "Epoch 80 Loss 0.0636\n",
      "Time taken for 1 epoch: 0.0770 secs\n",
      "\n",
      "Epoch 81 Loss 0.0627\n",
      "Time taken for 1 epoch: 0.0764 secs\n",
      "\n",
      "Epoch 82 Loss 0.0623\n",
      "Time taken for 1 epoch: 0.0646 secs\n",
      "\n",
      "Epoch 83 Loss 0.0611\n",
      "Time taken for 1 epoch: 0.0620 secs\n",
      "\n",
      "Epoch 84 Loss 0.0601\n",
      "Time taken for 1 epoch: 0.0697 secs\n",
      "\n",
      "Epoch 85 Loss 0.0596\n",
      "Time taken for 1 epoch: 0.0638 secs\n",
      "\n",
      "Epoch 86 Loss 0.0594\n",
      "Time taken for 1 epoch: 0.0620 secs\n",
      "\n",
      "Epoch 87 Loss 0.0586\n",
      "Time taken for 1 epoch: 0.0729 secs\n",
      "\n",
      "Epoch 88 Loss 0.0577\n",
      "Time taken for 1 epoch: 0.0644 secs\n",
      "\n",
      "Epoch 89 Loss 0.0568\n",
      "Time taken for 1 epoch: 0.0618 secs\n",
      "\n",
      "Epoch 90 Loss 0.0563\n",
      "Time taken for 1 epoch: 0.0647 secs\n",
      "\n",
      "Epoch 91 Loss 0.0555\n",
      "Time taken for 1 epoch: 0.0671 secs\n",
      "\n",
      "Epoch 92 Loss 0.0546\n",
      "Time taken for 1 epoch: 0.0651 secs\n",
      "\n",
      "Epoch 93 Loss 0.0541\n",
      "Time taken for 1 epoch: 0.0643 secs\n",
      "\n",
      "Epoch 94 Loss 0.0534\n",
      "Time taken for 1 epoch: 0.0610 secs\n",
      "\n",
      "Epoch 95 Loss 0.0526\n",
      "Time taken for 1 epoch: 0.0710 secs\n",
      "\n",
      "Epoch 96 Loss 0.0519\n",
      "Time taken for 1 epoch: 0.0749 secs\n",
      "\n",
      "Epoch 97 Loss 0.0514\n",
      "Time taken for 1 epoch: 0.0745 secs\n",
      "\n",
      "Epoch 98 Loss 0.0509\n",
      "Time taken for 1 epoch: 0.0726 secs\n",
      "\n",
      "Epoch 99 Loss 0.0501\n",
      "Time taken for 1 epoch: 0.0724 secs\n",
      "\n",
      "Epoch 100 Loss 0.0497\n",
      "Time taken for 1 epoch: 0.0672 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, tar) in enumerate(train_dataset):\n",
    "        train_step(tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                          ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "#                                                 train_accuracy.result()\n",
    "                                                        )\n",
    "          )\n",
    "\n",
    "    print ('Time taken for 1 epoch: {:.4f} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Need to incorporate gym env 'done' as end token\n",
    "\n",
    "def evaluate(inp_sequence):\n",
    "    output = tf.expand_dims(inp_sequence, 0)\n",
    "\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        look_ahead_mask = create_masks(inp_sequence)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(inp_sequence, \n",
    "                                                     False,\n",
    "                                                     look_ahead_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.reduce_sum(tf.cast(predictions, tf.float32))==0:\n",
    "            return predictions, attention_weights \n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "#         output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_state(sequence, plot=''):\n",
    "    predicted_sequence, attention_weights = evaluate(sequence)\n",
    "\n",
    "    print('Input: {}'.format(sequence))\n",
    "    print('Predicted translation: {}'.format(predicted_sequence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sequence, predicted_sequence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_trajectory = tf.expand_dims(iter(train_dataset.take(1)).next()[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[[-0.04684773  0.00488002  0.04090648 -0.00326716]\n",
      "  [-0.04675013 -0.19080399  0.04084114  0.30203646]\n",
      "  [-0.05056621 -0.38648352  0.04688187  0.60731494]\n",
      "  [-0.05829588 -0.58222854  0.05902817  0.91438806]\n",
      "  [-0.06994046 -0.77809703  0.07731593  1.2250234 ]\n",
      "  [-0.08550239 -0.58405113  0.10181639  0.9575322 ]\n",
      "  [-0.09718341 -0.7803839   0.12096704  1.2803886 ]\n",
      "  [-0.11279109 -0.9768215   0.14657481  1.6083694 ]\n",
      "  [-0.13232753 -0.78370446  0.1787422   1.3647388 ]\n",
      "  [-0.14800161 -0.9805572   0.20603697  1.7075845 ]\n",
      "  [-0.16761276 -1.1773664   0.24018866  2.0567014 ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "Predicted translation: [[[-0.50892454  0.06192771 -0.31248084  0.8296047 ]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-869862d924c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'decoder_layer1_block1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-856c643e7834>\u001b[0m in \u001b[0;36mpredict_state\u001b[0;34m(sequence, plot)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-0bed888cd2ee>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(attention, sentence, result, layer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_pt' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_state(example_trajectory, plot='decoder_layer1_block1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.equal([ 0.0000000000,  0.000000000,  0.0000000000,  0.0000000000], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rl_venv': venv)",
   "language": "python",
   "name": "python37664bitrlvenvvenvd21d88e11ee043b9809cf63723909d6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
