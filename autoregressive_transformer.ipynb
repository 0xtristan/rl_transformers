{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "env  =  gym.make('CartPole-v0')\n",
    "\n",
    "games = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    observations = []\n",
    "    obs = env.reset()\n",
    "    observations.append(obs)\n",
    "    for t in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        \n",
    "        if done:\n",
    "#             print(f'Episode {i} finished at time {t}')\n",
    "            break\n",
    "    games.append(np.stack(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "WINDOW_SIZE = BATCH_SIZE\n",
    "\n",
    "def filter_max_length(x, max_length=WINDOW_SIZE):\n",
    "    return tf.shape(x)[0] <= max_length\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: games, output_types=tf.float32)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset = train_dataset.cache()\n",
    "# train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes=(None,None))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 4), dtype=float32, numpy=\n",
       "array([[[-0.04515691, -0.01837284,  0.04578161,  0.03413424],\n",
       "        [-0.04552436,  0.1760637 ,  0.0464643 , -0.2437599 ],\n",
       "        [-0.04200309, -0.01969006,  0.0415891 ,  0.06320944],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.04276861, -0.02536054, -0.03682623,  0.01933085],\n",
       "        [-0.04327581, -0.21993555, -0.03643961,  0.30017102],\n",
       "        [-0.04767453, -0.41451967, -0.03043619,  0.58114254],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.02133315, -0.02512128,  0.01652734, -0.04125911],\n",
       "        [ 0.02083072,  0.16975982,  0.01570216, -0.32868204],\n",
       "        [ 0.02422592, -0.02558211,  0.00912852, -0.03108901],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.04723867,  0.04567653, -0.02058217, -0.02249833],\n",
       "        [ 0.0481522 ,  0.24108751, -0.02103214, -0.32160345],\n",
       "        [ 0.05297395,  0.43650255, -0.02746421, -0.6208442 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.04440615,  0.00863826,  0.02419653,  0.03991487],\n",
       "        [-0.04423338, -0.18682216,  0.02499483,  0.34013277],\n",
       "        [-0.04796982,  0.0079354 ,  0.03179748,  0.05543539],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.04199908,  0.0390625 ,  0.0249751 , -0.04916881],\n",
       "        [-0.04121783,  0.23381759,  0.02399172, -0.33386844],\n",
       "        [-0.03654148,  0.03836254,  0.01731435, -0.03371733],\n",
       "        ...,\n",
       "        [-0.09392823, -0.5709925 ,  0.1986376 ,  1.3975278 ],\n",
       "        [-0.10534807, -0.37881655,  0.22658816,  1.1729484 ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query*key gives the relevance measure - the better matched a key is to the query the higher it will activate and thus it will feature more in the v output\n",
    "# V[Q*K]\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "                    to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)    # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(k.shape[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)    \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)    # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "                \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)    # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)    # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)    # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)    # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)    # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)    # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])    # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)    # (batch_size, seq_len_q, d_model)\n",
    "                \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 10, 32]), TensorShape([1, 1, 10, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPR_DIM = 32 # These are just for testing\n",
    "SEQ_LEN = 10\n",
    "\n",
    "temp_mha = MultiHeadAttention(d_model=REPR_DIM, num_heads=1)\n",
    "y = tf.random.uniform((1, SEQ_LEN, REPR_DIM))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class DecoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "\n",
    "#         self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "#         # Decoder self-attention\n",
    "#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn1 = self.dropout1(attn1, training=training)\n",
    "#         out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "#         # Encoder-Decoder attention (v,k,q)\n",
    "#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn2 = self.dropout2(attn2, training=training)\n",
    "#         out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "#         ffn_output = self.dropout3(ffn_output, training=training)\n",
    "#         out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant of the decoder for decoder-only architectures like GPT-2 where we forgo an encoder\n",
    "class DecoderOnlyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderOnlyLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # Decoder masked self-attention\n",
    "        attn, attn_weights_block = self.mha(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        out = self.layernorm1(attn + x)\n",
    "\n",
    "        ffn_output = self.ffn(out)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out_final = self.layernorm2(ffn_output + out)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out_final, attn_weights_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderOnlyLayer(REPR_DIM, 1, REPR_DIM*4)\n",
    "\n",
    "sample_decoder_layer_output, _ = sample_decoder_layer(tf.random.uniform((64, SEQ_LEN, REPR_DIM)), False, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "$P E_{(p o s, 2 i)}=\\sin \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$\n",
    "\n",
    "$P E_{(p o s, 2 i+1)}=\\cos \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderOnlyLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "#         x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :] # would concat be better?\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, training, look_ahead_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10, 32]), TensorShape([64, 1, 10, 10]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=1, d_model=REPR_DIM, num_heads=1, \n",
    "                         dff=REPR_DIM*4, maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer1_block1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, pe_target, rate)\n",
    "\n",
    "    def call(self, tar, training, look_ahead_mask):\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, training, look_ahead_mask)\n",
    "\n",
    "        return dec_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=1, d_model=REPR_DIM, num_heads=1, dff=REPR_DIM*4, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, training=False, look_ahead_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask[:,:,0]\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar[:,:,0]) # drop the state dim\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "#                            optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "]\n",
    "\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(tar):\n",
    "    tar_inp = tar[:, :-1] # batch, time, dim\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    combined_mask = create_masks(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(tar_inp, \n",
    "                                     True, \n",
    "                                     combined_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "#     train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = 4\n",
    "transformer = Transformer(num_layers=1, d_model=OBS_DIM, num_heads=1, dff=OBS_DIM*4, pe_target=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.1946\n",
      "Time taken for 1 epoch: 0.7883 secs\n",
      "\n",
      "Epoch 2 Loss 0.1910\n",
      "Time taken for 1 epoch: 0.6994 secs\n",
      "\n",
      "Epoch 3 Loss 0.1871\n",
      "Time taken for 1 epoch: 0.7115 secs\n",
      "\n",
      "Epoch 4 Loss 0.1839\n",
      "Time taken for 1 epoch: 0.6985 secs\n",
      "\n",
      "Epoch 5 Loss 0.1803\n",
      "Time taken for 1 epoch: 0.6986 secs\n",
      "\n",
      "Epoch 6 Loss 0.1761\n",
      "Time taken for 1 epoch: 0.6796 secs\n",
      "\n",
      "Epoch 7 Loss 0.1730\n",
      "Time taken for 1 epoch: 0.6829 secs\n",
      "\n",
      "Epoch 8 Loss 0.1703\n",
      "Time taken for 1 epoch: 0.6893 secs\n",
      "\n",
      "Epoch 9 Loss 0.1671\n",
      "Time taken for 1 epoch: 0.6894 secs\n",
      "\n",
      "Epoch 10 Loss 0.1632\n",
      "Time taken for 1 epoch: 0.6876 secs\n",
      "\n",
      "Epoch 11 Loss 0.1616\n",
      "Time taken for 1 epoch: 0.7085 secs\n",
      "\n",
      "Epoch 12 Loss 0.1590\n",
      "Time taken for 1 epoch: 0.6941 secs\n",
      "\n",
      "Epoch 13 Loss 0.1551\n",
      "Time taken for 1 epoch: 0.6938 secs\n",
      "\n",
      "Epoch 14 Loss 0.1527\n",
      "Time taken for 1 epoch: 0.6846 secs\n",
      "\n",
      "Epoch 15 Loss 0.1506\n",
      "Time taken for 1 epoch: 0.6866 secs\n",
      "\n",
      "Epoch 16 Loss 0.1469\n",
      "Time taken for 1 epoch: 0.7033 secs\n",
      "\n",
      "Epoch 17 Loss 0.1457\n",
      "Time taken for 1 epoch: 0.6950 secs\n",
      "\n",
      "Epoch 18 Loss 0.1441\n",
      "Time taken for 1 epoch: 0.6879 secs\n",
      "\n",
      "Epoch 19 Loss 0.1415\n",
      "Time taken for 1 epoch: 0.6910 secs\n",
      "\n",
      "Epoch 20 Loss 0.1388\n",
      "Time taken for 1 epoch: 0.6880 secs\n",
      "\n",
      "Epoch 21 Loss 0.1368\n",
      "Time taken for 1 epoch: 0.6936 secs\n",
      "\n",
      "Epoch 22 Loss 0.1353\n",
      "Time taken for 1 epoch: 0.6866 secs\n",
      "\n",
      "Epoch 23 Loss 0.1328\n",
      "Time taken for 1 epoch: 0.7000 secs\n",
      "\n",
      "Epoch 24 Loss 0.1309\n",
      "Time taken for 1 epoch: 0.6916 secs\n",
      "\n",
      "Epoch 25 Loss 0.1293\n",
      "Time taken for 1 epoch: 0.7039 secs\n",
      "\n",
      "Epoch 26 Loss 0.1281\n",
      "Time taken for 1 epoch: 0.6962 secs\n",
      "\n",
      "Epoch 27 Loss 0.1254\n",
      "Time taken for 1 epoch: 0.7229 secs\n",
      "\n",
      "Epoch 28 Loss 0.1238\n",
      "Time taken for 1 epoch: 0.7180 secs\n",
      "\n",
      "Epoch 29 Loss 0.1227\n",
      "Time taken for 1 epoch: 0.7059 secs\n",
      "\n",
      "Epoch 30 Loss 0.1214\n",
      "Time taken for 1 epoch: 0.6899 secs\n",
      "\n",
      "Epoch 31 Loss 0.1194\n",
      "Time taken for 1 epoch: 0.6899 secs\n",
      "\n",
      "Epoch 32 Loss 0.1175\n",
      "Time taken for 1 epoch: 0.6848 secs\n",
      "\n",
      "Epoch 33 Loss 0.1163\n",
      "Time taken for 1 epoch: 0.6889 secs\n",
      "\n",
      "Epoch 34 Loss 0.1154\n",
      "Time taken for 1 epoch: 0.7000 secs\n",
      "\n",
      "Epoch 35 Loss 0.1131\n",
      "Time taken for 1 epoch: 0.6889 secs\n",
      "\n",
      "Epoch 36 Loss 0.1123\n",
      "Time taken for 1 epoch: 0.6956 secs\n",
      "\n",
      "Epoch 37 Loss 0.1104\n",
      "Time taken for 1 epoch: 0.6821 secs\n",
      "\n",
      "Epoch 38 Loss 0.1094\n",
      "Time taken for 1 epoch: 0.6853 secs\n",
      "\n",
      "Epoch 39 Loss 0.1080\n",
      "Time taken for 1 epoch: 0.6925 secs\n",
      "\n",
      "Epoch 40 Loss 0.1073\n",
      "Time taken for 1 epoch: 0.7075 secs\n",
      "\n",
      "Epoch 41 Loss 0.1064\n",
      "Time taken for 1 epoch: 0.7278 secs\n",
      "\n",
      "Epoch 42 Loss 0.1047\n",
      "Time taken for 1 epoch: 0.7147 secs\n",
      "\n",
      "Epoch 43 Loss 0.1034\n",
      "Time taken for 1 epoch: 0.7023 secs\n",
      "\n",
      "Epoch 44 Loss 0.1025\n",
      "Time taken for 1 epoch: 0.7154 secs\n",
      "\n",
      "Epoch 45 Loss 0.1008\n",
      "Time taken for 1 epoch: 0.6978 secs\n",
      "\n",
      "Epoch 46 Loss 0.0998\n",
      "Time taken for 1 epoch: 0.6929 secs\n",
      "\n",
      "Epoch 47 Loss 0.0989\n",
      "Time taken for 1 epoch: 0.7035 secs\n",
      "\n",
      "Epoch 48 Loss 0.0977\n",
      "Time taken for 1 epoch: 0.6968 secs\n",
      "\n",
      "Epoch 49 Loss 0.0967\n",
      "Time taken for 1 epoch: 0.7124 secs\n",
      "\n",
      "Epoch 50 Loss 0.0952\n",
      "Time taken for 1 epoch: 0.7035 secs\n",
      "\n",
      "Epoch 51 Loss 0.0949\n",
      "Time taken for 1 epoch: 0.6847 secs\n",
      "\n",
      "Epoch 52 Loss 0.0934\n",
      "Time taken for 1 epoch: 0.6856 secs\n",
      "\n",
      "Epoch 53 Loss 0.0925\n",
      "Time taken for 1 epoch: 0.7043 secs\n",
      "\n",
      "Epoch 54 Loss 0.0917\n",
      "Time taken for 1 epoch: 0.6935 secs\n",
      "\n",
      "Epoch 55 Loss 0.0900\n",
      "Time taken for 1 epoch: 0.6846 secs\n",
      "\n",
      "Epoch 56 Loss 0.0894\n",
      "Time taken for 1 epoch: 0.6859 secs\n",
      "\n",
      "Epoch 57 Loss 0.0883\n",
      "Time taken for 1 epoch: 0.7026 secs\n",
      "\n",
      "Epoch 58 Loss 0.0873\n",
      "Time taken for 1 epoch: 0.6903 secs\n",
      "\n",
      "Epoch 59 Loss 0.0860\n",
      "Time taken for 1 epoch: 0.6939 secs\n",
      "\n",
      "Epoch 60 Loss 0.0853\n",
      "Time taken for 1 epoch: 0.6881 secs\n",
      "\n",
      "Epoch 61 Loss 0.0847\n",
      "Time taken for 1 epoch: 0.6972 secs\n",
      "\n",
      "Epoch 62 Loss 0.0840\n",
      "Time taken for 1 epoch: 0.6830 secs\n",
      "\n",
      "Epoch 63 Loss 0.0827\n",
      "Time taken for 1 epoch: 0.6978 secs\n",
      "\n",
      "Epoch 64 Loss 0.0818\n",
      "Time taken for 1 epoch: 0.6954 secs\n",
      "\n",
      "Epoch 65 Loss 0.0810\n",
      "Time taken for 1 epoch: 0.6910 secs\n",
      "\n",
      "Epoch 66 Loss 0.0795\n",
      "Time taken for 1 epoch: 0.6969 secs\n",
      "\n",
      "Epoch 67 Loss 0.0789\n",
      "Time taken for 1 epoch: 0.7059 secs\n",
      "\n",
      "Epoch 68 Loss 0.0776\n",
      "Time taken for 1 epoch: 0.7068 secs\n",
      "\n",
      "Epoch 69 Loss 0.0772\n",
      "Time taken for 1 epoch: 0.6961 secs\n",
      "\n",
      "Epoch 70 Loss 0.0759\n",
      "Time taken for 1 epoch: 0.7119 secs\n",
      "\n",
      "Epoch 71 Loss 0.0745\n",
      "Time taken for 1 epoch: 0.7089 secs\n",
      "\n",
      "Epoch 72 Loss 0.0744\n",
      "Time taken for 1 epoch: 0.7077 secs\n",
      "\n",
      "Epoch 73 Loss 0.0733\n",
      "Time taken for 1 epoch: 0.6974 secs\n",
      "\n",
      "Epoch 74 Loss 0.0725\n",
      "Time taken for 1 epoch: 0.6984 secs\n",
      "\n",
      "Epoch 75 Loss 0.0715\n",
      "Time taken for 1 epoch: 0.6897 secs\n",
      "\n",
      "Epoch 76 Loss 0.0707\n",
      "Time taken for 1 epoch: 0.6936 secs\n",
      "\n",
      "Epoch 77 Loss 0.0696\n",
      "Time taken for 1 epoch: 0.6926 secs\n",
      "\n",
      "Epoch 78 Loss 0.0693\n",
      "Time taken for 1 epoch: 0.6886 secs\n",
      "\n",
      "Epoch 79 Loss 0.0683\n",
      "Time taken for 1 epoch: 0.6857 secs\n",
      "\n",
      "Epoch 80 Loss 0.0675\n",
      "Time taken for 1 epoch: 0.7026 secs\n",
      "\n",
      "Epoch 81 Loss 0.0664\n",
      "Time taken for 1 epoch: 0.7008 secs\n",
      "\n",
      "Epoch 82 Loss 0.0659\n",
      "Time taken for 1 epoch: 0.7002 secs\n",
      "\n",
      "Epoch 83 Loss 0.0650\n",
      "Time taken for 1 epoch: 0.7047 secs\n",
      "\n",
      "Epoch 84 Loss 0.0641\n",
      "Time taken for 1 epoch: 0.6974 secs\n",
      "\n",
      "Epoch 85 Loss 0.0635\n",
      "Time taken for 1 epoch: 0.6958 secs\n",
      "\n",
      "Epoch 86 Loss 0.0629\n",
      "Time taken for 1 epoch: 0.6979 secs\n",
      "\n",
      "Epoch 87 Loss 0.0622\n",
      "Time taken for 1 epoch: 0.6879 secs\n",
      "\n",
      "Epoch 88 Loss 0.0609\n",
      "Time taken for 1 epoch: 0.7002 secs\n",
      "\n",
      "Epoch 89 Loss 0.0606\n",
      "Time taken for 1 epoch: 0.6918 secs\n",
      "\n",
      "Epoch 90 Loss 0.0594\n",
      "Time taken for 1 epoch: 0.7042 secs\n",
      "\n",
      "Epoch 91 Loss 0.0593\n",
      "Time taken for 1 epoch: 0.7037 secs\n",
      "\n",
      "Epoch 92 Loss 0.0585\n",
      "Time taken for 1 epoch: 0.7138 secs\n",
      "\n",
      "Epoch 93 Loss 0.0576\n",
      "Time taken for 1 epoch: 0.6907 secs\n",
      "\n",
      "Epoch 94 Loss 0.0570\n",
      "Time taken for 1 epoch: 0.6871 secs\n",
      "\n",
      "Epoch 95 Loss 0.0562\n",
      "Time taken for 1 epoch: 0.7023 secs\n",
      "\n",
      "Epoch 96 Loss 0.0558\n",
      "Time taken for 1 epoch: 0.7051 secs\n",
      "\n",
      "Epoch 97 Loss 0.0548\n",
      "Time taken for 1 epoch: 0.6989 secs\n",
      "\n",
      "Epoch 98 Loss 0.0540\n",
      "Time taken for 1 epoch: 0.6869 secs\n",
      "\n",
      "Epoch 99 Loss 0.0534\n",
      "Time taken for 1 epoch: 0.6917 secs\n",
      "\n",
      "Epoch 100 Loss 0.0530\n",
      "Time taken for 1 epoch: 0.6989 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, tar) in enumerate(train_dataset):\n",
    "        train_step(tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                          ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "#                                                 train_accuracy.result()\n",
    "                                                        )\n",
    "          )\n",
    "\n",
    "    print ('Time taken for 1 epoch: {:.4f} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Need to incorporate gym env 'done' as end token\n",
    "# This eval takes in t=0 state and keeps generating new states, feeding it's predicted output back in at every step\n",
    "# This is essentially GPT2 unconditional autoregressive generation operating in open loop mode - rather than closed loop teacher forcin\n",
    "\n",
    "def evaluate(inp_sequence):\n",
    "    output = tf.expand_dims(tf.expand_dims(inp_sequence[0],0),1) # seed input at t=0, now we generate the rest\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        look_ahead_mask = create_masks(output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(output, \n",
    "                                                     False,\n",
    "                                                     look_ahead_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.reduce_sum(tf.cast(predictions, tf.float32))==0:\n",
    "            return tf.squeeze(output,0), attention_weights \n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predictions], axis=1)\n",
    "\n",
    "    return tf.squeeze(output,0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots the attention weights which signify how the output timesteps attend to the input timesteps\n",
    "# For data that is linearly correlated in time we expect a diagonal matrix\n",
    "def plot_attention_weights(attention, sequence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "#     sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0) # layer, head, time, time\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "#         ax.set_xticks(range(len(sequence)+2))\n",
    "#         ax.set_yticks(range(len(result)))\n",
    "\n",
    "#         ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "#         ax.set_xticklabels(\n",
    "#             ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "#             fontdict=fontdict, rotation=90)\n",
    "\n",
    "#         ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "#                             if i < tokenizer_en.vocab_size], \n",
    "#                            fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_state(sequence, plot=''):\n",
    "    predicted_sequence, attention_weights = evaluate(sequence)\n",
    "\n",
    "    print('Input: {}'.format(sequence))\n",
    "    print('Predicted translation: {}'.format(predicted_sequence))\n",
    "    \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sequence, predicted_sequence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 4])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_trajectory = iter(train_dataset.take(1)).next()[0]\n",
    "example_trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-0.04515691 -0.01837284  0.04578161  0.03413424]\n",
      " [-0.04552436  0.1760637   0.0464643  -0.2437599 ]\n",
      " [-0.04200309 -0.01969006  0.0415891   0.06320944]\n",
      " [-0.04239689 -0.21538286  0.04285329  0.36871848]\n",
      " [-0.04670455 -0.41108668  0.05022766  0.6745997 ]\n",
      " [-0.05492628 -0.60686934  0.06371965  0.9826641 ]\n",
      " [-0.06706367 -0.80278456  0.08337294  1.2946614 ]\n",
      " [-0.08311936 -0.608815    0.10926616  1.0292009 ]\n",
      " [-0.09529566 -0.41530332  0.12985018  0.77272487]\n",
      " [-0.10360172 -0.22218426  0.14530468  0.5235537 ]\n",
      " [-0.10804541 -0.4190202   0.15577576  0.85826564]\n",
      " [-0.11642581 -0.61588204  0.17294106  1.1955959 ]\n",
      " [-0.12874345 -0.42336828  0.19685298  0.96172446]\n",
      " [-0.13721082 -0.6205127   0.21608748  1.3092316 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Predicted translation: [[-0.04515691 -0.01837284  0.04578161  0.03413424]\n",
      " [-0.35730654  0.31946126 -0.5177133   0.7506982 ]\n",
      " [-0.3375468   0.03451543 -0.4025684   0.90208375]\n",
      " [-0.20512232 -0.39464748 -0.20915242  1.0079641 ]\n",
      " [-0.10982414 -0.63977545 -0.04966247  0.9997448 ]\n",
      " [-0.14635293 -0.66752064  0.02220415  0.99189913]\n",
      " [-0.18685685 -0.63370556  0.02519185  0.9951776 ]\n",
      " [-0.1907155  -0.5935823  -0.01837879  1.0023623 ]\n",
      " [-0.13708697 -0.59943944 -0.068212    1.004897  ]\n",
      " [-0.11394516 -0.6330616  -0.05329393  1.0007334 ]\n",
      " [-0.10330583 -0.6792404  -0.00912891  0.9922992 ]\n",
      " [-0.1448209  -0.67866164  0.03448943  0.9892587 ]\n",
      " [-0.18404531 -0.6572117   0.05122543  0.9899116 ]\n",
      " [-0.19862324 -0.61845744  0.01982754  0.99692476]\n",
      " [-0.15886903 -0.6101859  -0.03326569  1.0023185 ]\n",
      " [-0.10823643 -0.63941395 -0.0516964   0.9998423 ]\n",
      " [-0.09780163 -0.676579   -0.01807447  0.9931205 ]\n",
      " [-0.13489644 -0.68231505  0.02825874  0.98931175]\n",
      " [-0.17827764 -0.6661767   0.05603115  0.9883717 ]\n",
      " [-0.20196196 -0.63388336  0.04247136  0.99304885]\n",
      " [-0.17744902 -0.6146984  -0.00804838  1.0000436 ]\n",
      " [-0.1201929  -0.6355665  -0.04399106  1.0001359 ]\n",
      " [-0.0986391  -0.67081827 -0.02420007  0.99430335]\n",
      " [-0.12409063 -0.68539965  0.02046618  0.9894827 ]\n",
      " [-0.17037989 -0.67155784  0.05393633  0.9880297 ]\n",
      " [-0.20014708 -0.6480509   0.05809406  0.98982364]\n",
      " [-0.19163138 -0.62145454  0.01563316  0.997192  ]\n",
      " [-0.13735178 -0.63127893 -0.03124127  1.0001016 ]\n",
      " [-0.10003997 -0.6656523  -0.02898796  0.99530315]\n",
      " [-0.11342297 -0.68774086  0.01201394  0.989705  ]\n",
      " [-0.16002919 -0.6767455   0.04891107  0.9879928 ]\n",
      " [-0.19574289 -0.6587056   0.06653044  0.9876979 ]\n",
      " [-0.20114625 -0.63021946  0.03701995  0.9940203 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_state(example_trajectory, plot='decoder_layer1_block1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rl_venv': venv)",
   "language": "python",
   "name": "python37664bitrlvenvvenvd21d88e11ee043b9809cf63723909d6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
