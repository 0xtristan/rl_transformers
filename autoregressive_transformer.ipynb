{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "env  =  gym.make('CartPole-v0')\n",
    "\n",
    "games = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    observations = []\n",
    "    obs = env.reset()\n",
    "    observations.append(obs)\n",
    "    for t in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        \n",
    "        if done:\n",
    "#             print(f'Episode {i} finished at time {t}')\n",
    "            break\n",
    "    games.append(np.stack(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "WINDOW_SIZE = 32\n",
    "\n",
    "def filter_max_length(x, max_length=WINDOW_SIZE):\n",
    "    return tf.shape(x)[0] <= max_length\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: games, output_types=tf.float32)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes=(None,None))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32, 4), dtype=float32, numpy=\n",
       "array([[[ 0.03114268,  0.01231103,  0.0096411 ,  0.02125562],\n",
       "        [ 0.0313889 , -0.18294784,  0.01006621,  0.31696478],\n",
       "        [ 0.02772995, -0.3782117 ,  0.01640551,  0.6128051 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.00963965,  0.02486031,  0.01664675,  0.04907015],\n",
       "        [-0.00914244,  0.21973965,  0.01762816, -0.23831443],\n",
       "        [-0.00474765,  0.02437036,  0.01286187,  0.05987637],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.00592813,  0.04643025,  0.03915752,  0.00755509],\n",
       "        [ 0.00685673, -0.14923076,  0.03930862,  0.312331  ],\n",
       "        [ 0.00387212,  0.04530978,  0.04555524,  0.03229936],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.01777415, -0.037213  , -0.04893858,  0.04495578],\n",
       "        [-0.01851841, -0.23160028, -0.04803946,  0.3218054 ],\n",
       "        [-0.02315042, -0.42600638, -0.04160336,  0.59896   ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.00802337,  0.003312  , -0.03640743, -0.00915485],\n",
       "        [-0.00795713,  0.19893666, -0.03659053, -0.3130988 ],\n",
       "        [-0.0039784 ,  0.00435456, -0.04285251, -0.03217623],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.0316229 ,  0.01144947, -0.02372698, -0.03086748],\n",
       "        [-0.03139391, -0.18332432, -0.02434433,  0.2542359 ],\n",
       "        [-0.03506039, -0.37809038, -0.01925961,  0.5391419 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "# calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query*key gives the relevance measure - the better matched a key is to the query the higher it will activate and thus it will feature more in the v output\n",
    "# V[Q*K]\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "                    to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)    # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(k.shape[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)    \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)    # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "                \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)    # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)    # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)    # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)    # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)    # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)    # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])    # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)    # (batch_size, seq_len_q, d_model)\n",
    "                \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 10, 32]), TensorShape([1, 1, 10, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPR_DIM = 32 # These are just for testing\n",
    "SEQ_LEN = 10\n",
    "\n",
    "temp_mha = MultiHeadAttention(d_model=REPR_DIM, num_heads=1)\n",
    "y = tf.random.uniform((1, SEQ_LEN, REPR_DIM))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class DecoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "\n",
    "#         self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "#         self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "#         # Decoder self-attention\n",
    "#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn1 = self.dropout1(attn1, training=training)\n",
    "#         out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "#         # Encoder-Decoder attention (v,k,q)\n",
    "#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "#         attn2 = self.dropout2(attn2, training=training)\n",
    "#         out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "#         ffn_output = self.dropout3(ffn_output, training=training)\n",
    "#         out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "#         return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant of the decoder for decoder-only architectures like GPT-2 where we forgo an encoder\n",
    "class DecoderOnlyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderOnlyLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # Decoder masked self-attention\n",
    "        attn, attn_weights_block = self.mha(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        out = self.layernorm1(attn + x)\n",
    "\n",
    "        ffn_output = self.ffn(out)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out_final = self.layernorm2(ffn_output + out)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out_final, attn_weights_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderOnlyLayer(REPR_DIM, 1, REPR_DIM*4)\n",
    "\n",
    "sample_decoder_layer_output, _ = sample_decoder_layer(tf.random.uniform((64, SEQ_LEN, REPR_DIM)), False, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "$P E_{(p o s, 2 i)}=\\sin \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$\n",
    "\n",
    "$P E_{(p o s, 2 i+1)}=\\cos \\left(p o s / 10000^{2 i / d_{m o d e l}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderOnlyLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "#         x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :] # would concat be better?\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1 = self.dec_layers[i](x, training, look_ahead_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10, 32]), TensorShape([64, 1, 10, 10]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=1, d_model=REPR_DIM, num_heads=1, \n",
    "                         dff=REPR_DIM*4, maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer1_block1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, pe_target, rate)\n",
    "\n",
    "    def call(self, tar, training, look_ahead_mask):\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, training, look_ahead_mask)\n",
    "\n",
    "        return dec_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 10, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=1, d_model=REPR_DIM, num_heads=1, dff=REPR_DIM*4, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, SEQ_LEN, REPR_DIM), dtype=tf.float32, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, training=False, look_ahead_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask[:,:,0]\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar[:,:,0]) # drop the state dim\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "#                            optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(tar):\n",
    "    tar_inp = tar[:, :-1] # batch, time, dim\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    combined_mask = create_masks(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(tar_inp, \n",
    "                                     True, \n",
    "                                     combined_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "#     train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = env.observation_space.shape[0]\n",
    "# num_heads has to be a factor of OBS_DIM\n",
    "transformer = Transformer(num_layers=1, d_model=OBS_DIM, num_heads=2, dff=OBS_DIM*4, pe_target=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 32s 635ms/epoch - train loss: 0.0376 - time/ep: 0.6336\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(EPOCHS, width=30, verbose=1, interval=0.1, unit_name='epoch')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, tar) in enumerate(train_dataset):\n",
    "        train_step(tar)\n",
    "\n",
    "#     if batch % 50 == 0:\n",
    "#         print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "#             epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                          ckpt_save_path))\n",
    "\n",
    "#     print ('Epoch {} Loss {:.4f}'.format(epoch + 1, \n",
    "#                                                 train_loss.result(), \n",
    "# #                                                 train_accuracy.result()\n",
    "#                                                         )\n",
    "#           )\n",
    "\n",
    "#     print ('Time taken for 1 epoch: {:.4f} secs\\n'.format(time.time() - start))\n",
    "\n",
    "    progbar.add(1, values=[(\"train loss\", train_loss.result()), (\"time/ep\", round(time.time() - start,4) )]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Need to incorporate gym env 'done' as end token\n",
    "# This eval takes in t=0 state and keeps generating new states, feeding it's predicted output back in at every step\n",
    "# This is essentially GPT2 unconditional autoregressive generation operating in open loop mode - rather than closed loop teacher forcin\n",
    "\n",
    "def evaluate_unconditional(inp_sequence):\n",
    "    output = tf.expand_dims(tf.expand_dims(inp_sequence[0],0),1) # seed input at t=0, now we generate the rest\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    for i in range(WINDOW_SIZE):\n",
    "        look_ahead_mask = create_masks(output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(output, \n",
    "                                                     False,\n",
    "                                                     look_ahead_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.reduce_sum(tf.cast(predictions, tf.float32))==0:\n",
    "            return tf.squeeze(output,0), attention_weights \n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predictions], axis=1)\n",
    "\n",
    "    return tf.squeeze(output,0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sequence):\n",
    "    # Here is boolean mask the input trajectory so we physically remove padded states\n",
    "    inp_sequence = tf.boolean_mask(inp_sequence,tf.reduce_max(tf.cast(inp_sequence!=0,tf.float32),axis=1), axis=0)\n",
    "    output = tf.expand_dims(inp_sequence,0) # seed input at t=0, now we generate the rest\n",
    "\n",
    "    look_ahead_mask = create_masks(output)\n",
    "\n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(output, \n",
    "                                                 False,\n",
    "                                                 look_ahead_mask)\n",
    "\n",
    "    return tf.squeeze(predictions,0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots the attention weights which signify how the output timesteps attend to the input timesteps\n",
    "# For data that is linearly correlated in time we expect a diagonal matrix\n",
    "# x:input, y:output\n",
    "def plot_attention_weights(attention, sequence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "#     sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0) # layer, head, time, time\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "#         ax.set_xticks(range(len(sequence)+2))\n",
    "#         ax.set_yticks(range(len(result)))\n",
    "\n",
    "#         ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "#         ax.set_xticklabels(\n",
    "#             ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "#             fontdict=fontdict, rotation=90)\n",
    "\n",
    "#         ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "#                             if i < tokenizer_en.vocab_size], \n",
    "#                            fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_state(sequence, plot=''):\n",
    "    predicted_sequence, attention_weights = evaluate(sequence)\n",
    "\n",
    "    print('Input: {}'.format(sequence))\n",
    "    print('Predicted translation: {}'.format(predicted_sequence))\n",
    "    \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sequence, predicted_sequence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 4])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_trajectory = iter(train_dataset.take(1)).next()[1]\n",
    "example_trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[ 3.33089032e-03  2.50934195e-02  1.73656773e-02 -3.41044292e-02]\n",
      " [ 3.83275864e-03 -1.70273200e-01  1.66835878e-02  2.64006555e-01]\n",
      " [ 4.27294668e-04 -3.65629256e-01  2.19637193e-02  5.61904609e-01]\n",
      " [-6.88529061e-03 -5.61052442e-01  3.32018137e-02  8.61425519e-01]\n",
      " [-1.81063395e-02 -3.66397977e-01  5.04303239e-02  5.79364240e-01]\n",
      " [-2.54342984e-02 -1.72017664e-01  6.20176084e-02  3.02984715e-01]\n",
      " [-2.88746525e-02  2.21681111e-02  6.80773035e-02  3.04875225e-02]\n",
      " [-2.84312908e-02  2.16251090e-01  6.86870515e-02 -2.39963338e-01]\n",
      " [-2.41062678e-02  4.10328031e-01  6.38877824e-02 -5.10214150e-01]\n",
      " [-1.58997066e-02  2.14366987e-01  5.36835045e-02 -1.98102042e-01]\n",
      " [-1.16123678e-02  1.85199287e-02  4.97214608e-02  1.11020707e-01]\n",
      " [-1.12419687e-02 -1.77277938e-01  5.19418754e-02  4.18966621e-01]\n",
      " [-1.47875277e-02 -3.73095959e-01  6.03212081e-02  7.27561593e-01]\n",
      " [-2.22494472e-02 -1.78857520e-01  7.48724416e-02  4.54457521e-01]\n",
      " [-2.58265976e-02 -3.74953896e-01  8.39615911e-02  7.69769907e-01]\n",
      " [-3.33256759e-02 -5.71124911e-01  9.93569866e-02  1.08764553e+00]\n",
      " [-4.47481722e-02 -3.77443463e-01  1.21109895e-01  8.27719152e-01]\n",
      " [-5.22970408e-02 -1.84167147e-01  1.37664288e-01  5.75447559e-01]\n",
      " [-5.59803843e-02  8.78380984e-03  1.49173230e-01  3.29104900e-01]\n",
      " [-5.58047108e-02  2.01502234e-01  1.55755326e-01  8.69321525e-02]\n",
      " [-5.17746657e-02  4.53044008e-03  1.57493979e-01  4.24420357e-01]\n",
      " [-5.16840555e-02 -1.92430943e-01  1.65982381e-01  7.62317896e-01]\n",
      " [-5.55326752e-02 -3.89402747e-01  1.81228742e-01  1.10229003e+00]\n",
      " [-6.33207262e-02 -5.86385369e-01  2.03274533e-01  1.44591510e+00]\n",
      " [-7.50484392e-02 -3.94260168e-01  2.32192844e-01  1.22301280e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "Predicted translation: [[-2.20951512e-02  1.04506016e-01  3.11697721e-02 -5.76177239e-02]\n",
      " [-2.49760449e-02 -3.88902426e-02  3.87965441e-02  1.65489197e-01]\n",
      " [-2.32460350e-02 -2.74142832e-01  4.00762819e-02  5.21492839e-01]\n",
      " [-6.25543296e-03 -5.93023777e-01  1.19342739e-02  1.01379704e+00]\n",
      " [-1.68295056e-02 -4.17974681e-01  2.54318453e-02  7.75921822e-01]\n",
      " [-2.44770199e-02 -2.27916420e-01  4.32646088e-02  4.38963830e-01]\n",
      " [-2.51408666e-02 -7.66180754e-02  3.90223861e-02  2.28800178e-01]\n",
      " [-2.03999579e-02  1.37748480e-01  2.01497190e-02 -5.57922721e-02]\n",
      " [-1.43458471e-02  2.65991867e-01  9.42173973e-03 -2.76552171e-01]\n",
      " [-2.57050619e-02 -7.12745786e-02  4.58641052e-02  1.77520096e-01]\n",
      " [-2.46609300e-02 -2.09922373e-01  4.25848663e-02  4.16774988e-01]\n",
      " [-2.36691236e-02 -2.53469348e-01  4.01930735e-02  4.92419004e-01]\n",
      " [-2.20135897e-02 -3.14741552e-01  3.78723592e-02  5.87390661e-01]\n",
      " [-2.42645591e-02 -1.99337095e-01  3.79785784e-02  4.27527130e-01]\n",
      " [-1.70437098e-02 -4.24929887e-01  2.80850977e-02  7.70949364e-01]\n",
      " [ 6.85930252e-04 -6.86826348e-01  5.26914932e-03  1.11870599e+00]\n",
      " [-7.86609948e-03 -5.65359890e-01  1.27297221e-02  9.85215247e-01]\n",
      " [-1.94241554e-02 -3.63373160e-01  2.98317298e-02  6.90187514e-01]\n",
      " [-2.55075991e-02 -1.76583618e-01  4.70043495e-02  3.41001630e-01]\n",
      " [-2.46632174e-02 -2.13029981e-02  3.66076007e-02  1.47968888e-01]\n",
      " [-2.36571580e-02 -2.41779208e-01  3.84190977e-02  4.85952020e-01]\n",
      " [-1.39616579e-02 -4.82575715e-01  2.36922484e-02  8.54662955e-01]\n",
      " [-4.18916345e-04 -6.64970219e-01  4.36380878e-03  1.10503960e+00]\n",
      " [ 7.48951733e-03 -7.54828393e-01 -3.99616919e-03  1.20199370e+00]\n",
      " [-7.64732063e-03 -5.73975444e-01  1.37366615e-02  9.88922477e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_state(example_trajectory, plot='decoder_layer1_block1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[ 3.33089032e-03  2.50934195e-02  1.73656773e-02 -3.41044292e-02]\n",
      " [ 3.83275864e-03 -1.70273200e-01  1.66835878e-02  2.64006555e-01]\n",
      " [ 4.27294668e-04 -3.65629256e-01  2.19637193e-02  5.61904609e-01]\n",
      " [-6.88529061e-03 -5.61052442e-01  3.32018137e-02  8.61425519e-01]\n",
      " [-1.81063395e-02 -3.66397977e-01  5.04303239e-02  5.79364240e-01]\n",
      " [-2.54342984e-02 -1.72017664e-01  6.20176084e-02  3.02984715e-01]\n",
      " [-2.88746525e-02  2.21681111e-02  6.80773035e-02  3.04875225e-02]\n",
      " [-2.84312908e-02  2.16251090e-01  6.86870515e-02 -2.39963338e-01]\n",
      " [-2.41062678e-02  4.10328031e-01  6.38877824e-02 -5.10214150e-01]\n",
      " [-1.58997066e-02  2.14366987e-01  5.36835045e-02 -1.98102042e-01]\n",
      " [-1.16123678e-02  1.85199287e-02  4.97214608e-02  1.11020707e-01]\n",
      " [-1.12419687e-02 -1.77277938e-01  5.19418754e-02  4.18966621e-01]\n",
      " [-1.47875277e-02 -3.73095959e-01  6.03212081e-02  7.27561593e-01]\n",
      " [-2.22494472e-02 -1.78857520e-01  7.48724416e-02  4.54457521e-01]\n",
      " [-2.58265976e-02 -3.74953896e-01  8.39615911e-02  7.69769907e-01]\n",
      " [-3.33256759e-02 -5.71124911e-01  9.93569866e-02  1.08764553e+00]\n",
      " [-4.47481722e-02 -3.77443463e-01  1.21109895e-01  8.27719152e-01]\n",
      " [-5.22970408e-02 -1.84167147e-01  1.37664288e-01  5.75447559e-01]\n",
      " [-5.59803843e-02  8.78380984e-03  1.49173230e-01  3.29104900e-01]\n",
      " [-5.58047108e-02  2.01502234e-01  1.55755326e-01  8.69321525e-02]\n",
      " [-5.17746657e-02  4.53044008e-03  1.57493979e-01  4.24420357e-01]\n",
      " [-5.16840555e-02 -1.92430943e-01  1.65982381e-01  7.62317896e-01]\n",
      " [-5.55326752e-02 -3.89402747e-01  1.81228742e-01  1.10229003e+00]\n",
      " [-6.33207262e-02 -5.86385369e-01  2.03274533e-01  1.44591510e+00]\n",
      " [-7.50484392e-02 -3.94260168e-01  2.32192844e-01  1.22301280e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "Predicted translation: [[-2.20951512e-02  1.04506016e-01  3.11697721e-02 -5.76177239e-02]\n",
      " [-2.49760449e-02 -3.88902426e-02  3.87965441e-02  1.65489197e-01]\n",
      " [-2.32460350e-02 -2.74142832e-01  4.00762819e-02  5.21492839e-01]\n",
      " [-6.25543296e-03 -5.93023777e-01  1.19342739e-02  1.01379704e+00]\n",
      " [-1.68295056e-02 -4.17974681e-01  2.54318453e-02  7.75921822e-01]\n",
      " [-2.44770199e-02 -2.27916420e-01  4.32646088e-02  4.38963830e-01]\n",
      " [-2.51408666e-02 -7.66180754e-02  3.90223861e-02  2.28800178e-01]\n",
      " [-2.03999579e-02  1.37748480e-01  2.01497190e-02 -5.57922721e-02]\n",
      " [-1.43458471e-02  2.65991867e-01  9.42173973e-03 -2.76552171e-01]\n",
      " [-2.57050619e-02 -7.12745786e-02  4.58641052e-02  1.77520096e-01]\n",
      " [-2.46609300e-02 -2.09922373e-01  4.25848663e-02  4.16774988e-01]\n",
      " [-2.36691236e-02 -2.53469348e-01  4.01930735e-02  4.92419004e-01]\n",
      " [-2.20135897e-02 -3.14741552e-01  3.78723592e-02  5.87390661e-01]\n",
      " [-2.42645591e-02 -1.99337095e-01  3.79785784e-02  4.27527130e-01]\n",
      " [-1.70437098e-02 -4.24929887e-01  2.80850977e-02  7.70949364e-01]\n",
      " [ 6.85930252e-04 -6.86826348e-01  5.26914932e-03  1.11870599e+00]\n",
      " [-7.86609948e-03 -5.65359890e-01  1.27297221e-02  9.85215247e-01]\n",
      " [-1.94241554e-02 -3.63373160e-01  2.98317298e-02  6.90187514e-01]\n",
      " [-2.55075991e-02 -1.76583618e-01  4.70043495e-02  3.41001630e-01]\n",
      " [-2.46632174e-02 -2.13029981e-02  3.66076007e-02  1.47968888e-01]\n",
      " [-2.36571580e-02 -2.41779208e-01  3.84190977e-02  4.85952020e-01]\n",
      " [-1.39616579e-02 -4.82575715e-01  2.36922484e-02  8.54662955e-01]\n",
      " [-4.18916345e-04 -6.64970219e-01  4.36380878e-03  1.10503960e+00]\n",
      " [ 7.48951733e-03 -7.54828393e-01 -3.99616919e-03  1.20199370e+00]\n",
      " [-7.64732063e-03 -5.73975444e-01  1.37366615e-02  9.88922477e-01]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decoder_layer2_block1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-ebd11dd594a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'decoder_layer2_block1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-15503823ad49>\u001b[0m in \u001b[0;36mpredict_state\u001b[0;34m(sequence, plot)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-47cff5e2f25f>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(attention, sequence, result, layer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     sentence = tokenizer_pt.encode(sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# layer, head, time, time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decoder_layer2_block1'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_state(example_trajectory, plot='decoder_layer2_block1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_17 (Decoder)         multiple                  244       \n",
      "=================================================================\n",
      "Total params: 244\n",
      "Trainable params: 244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rl_venv': venv)",
   "language": "python",
   "name": "python37664bitrlvenvvenvd21d88e11ee043b9809cf63723909d6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
